{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --trace-size 1000 --rtrace-length 500 --model 2 --weird --autoshroom --episode-length 200 --algo ppo --recurrent-policy --skip --use-linear-lr-decay --use-gae --lr 2.5e-5 --env-name \"Mario-v1\"  --clip-param 0.1 --value-loss-coef 0.5 --num-processes 20 --num-steps 40 --num-mini-batch 10 --log-interval 5  --entropy-coef 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-03 20:42:38.621004: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-03-03 20:42:38.621020: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Logging to /tmp/openai-2021-03-03-20-42-39-120677\n",
      "Creating dummy env object to get spaces\n",
      "/home/peli/retro-copy/MarioWM/a2c_ppo_acktr/model.py:155: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  has_zeros = ((masks[1:] == 0.0) \\\n",
      "Updates 4, num timesteps 600, FPS 86 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "/home/peli/retro-copy/retro-copy-venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/peli/retro-copy/retro-copy-venv/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Updates 6, num timesteps 840, FPS 86 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 8, num timesteps 1080, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 10, num timesteps 1320, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 238.7/233.0, min/max reward 221.0/262.0\n",
      "\n",
      "Updates 12, num timesteps 1560, FPS 86 \n",
      " Last 3 training episodes: mean/median reward 238.7/233.0, min/max reward 221.0/262.0\n",
      "\n",
      "Updates 14, num timesteps 1800, FPS 87 \n",
      " Last 3 training episodes: mean/median reward 259.0/273.0, min/max reward 218.0/286.0\n",
      "\n",
      "Updates 16, num timesteps 2040, FPS 88 \n",
      " Last 3 training episodes: mean/median reward 259.0/273.0, min/max reward 218.0/286.0\n",
      "\n",
      "Updates 18, num timesteps 2280, FPS 77 \n",
      " Last 3 training episodes: mean/median reward 259.0/273.0, min/max reward 218.0/286.0\n",
      "\n",
      "Updates 20, num timesteps 2520, FPS 72 \n",
      " Last 3 training episodes: mean/median reward 282.7/289.0, min/max reward 230.0/329.0\n",
      "\n",
      "Updates 22, num timesteps 2760, FPS 74 \n",
      " Last 3 training episodes: mean/median reward 282.7/289.0, min/max reward 230.0/329.0\n",
      "\n",
      "Updates 24, num timesteps 3000, FPS 75 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 26, num timesteps 3240, FPS 76 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 28, num timesteps 3480, FPS 77 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 30, num timesteps 3720, FPS 77 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 32, num timesteps 3960, FPS 78 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 34, num timesteps 4200, FPS 79 \n",
      " Last 3 training episodes: mean/median reward 224.3/223.0, min/max reward 218.0/232.0\n",
      "\n",
      "Updates 36, num timesteps 4440, FPS 79 \n",
      " Last 3 training episodes: mean/median reward 224.3/223.0, min/max reward 218.0/232.0\n",
      "\n",
      "Updates 38, num timesteps 4680, FPS 80 \n",
      " Last 3 training episodes: mean/median reward 224.3/223.0, min/max reward 218.0/232.0\n",
      "\n",
      "Updates 40, num timesteps 4920, FPS 80 \n",
      " Last 3 training episodes: mean/median reward 222.0/218.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 42, num timesteps 5160, FPS 81 \n",
      " Last 3 training episodes: mean/median reward 222.0/218.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 44, num timesteps 5400, FPS 82 \n",
      " Last 3 training episodes: mean/median reward 272.0/257.0, min/max reward 225.0/334.0\n",
      "\n",
      "Updates 46, num timesteps 5640, FPS 82 \n",
      " Last 3 training episodes: mean/median reward 272.0/257.0, min/max reward 225.0/334.0\n",
      "\n",
      "Updates 48, num timesteps 5880, FPS 82 \n",
      " Last 3 training episodes: mean/median reward 272.0/257.0, min/max reward 225.0/334.0\n",
      "\n",
      "Updates 50, num timesteps 6120, FPS 83 \n",
      " Last 3 training episodes: mean/median reward 271.3/287.0, min/max reward 218.0/309.0\n",
      "\n",
      "Updates 52, num timesteps 6360, FPS 83 \n",
      " Last 3 training episodes: mean/median reward 271.3/287.0, min/max reward 218.0/309.0\n",
      "\n",
      "Updates 54, num timesteps 6600, FPS 83 \n",
      " Last 3 training episodes: mean/median reward 222.0/218.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 56, num timesteps 6840, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 222.0/218.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 58, num timesteps 7080, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 60, num timesteps 7320, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 220.7/221.0, min/max reward 218.0/223.0\n",
      "\n",
      "Updates 62, num timesteps 7560, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 220.7/221.0, min/max reward 218.0/223.0\n",
      "\n",
      "Updates 64, num timesteps 7800, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 219.7/218.0, min/max reward 218.0/223.0\n",
      "\n",
      "Updates 66, num timesteps 8040, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 219.7/218.0, min/max reward 218.0/223.0\n",
      "\n",
      "Updates 68, num timesteps 8280, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 252.0/223.0, min/max reward 218.0/315.0\n",
      "\n",
      "Updates 70, num timesteps 8520, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 251.3/221.0, min/max reward 218.0/315.0\n",
      "\n",
      "Updates 72, num timesteps 8760, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 251.3/221.0, min/max reward 218.0/315.0\n",
      "\n",
      "Updates 74, num timesteps 9000, FPS 84 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 76, num timesteps 9240, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 78, num timesteps 9480, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 80, num timesteps 9720, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 82, num timesteps 9960, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 223.7/223.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 84, num timesteps 10200, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 271.3/281.0, min/max reward 230.0/303.0\n",
      "\n",
      "Updates 86, num timesteps 10440, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 271.3/281.0, min/max reward 230.0/303.0\n",
      "\n",
      "Updates 88, num timesteps 10680, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 243.0/230.0, min/max reward 218.0/281.0\n",
      "\n",
      "Updates 90, num timesteps 10920, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 92, num timesteps 11160, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 94, num timesteps 11400, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 236.7/232.0, min/max reward 218.0/260.0\n",
      "\n",
      "Updates 96, num timesteps 11640, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 236.7/232.0, min/max reward 218.0/260.0\n",
      "\n",
      "Updates 98, num timesteps 11880, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 236.3/232.0, min/max reward 218.0/259.0\n",
      "\n",
      "Updates 100, num timesteps 12120, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 235.7/230.0, min/max reward 218.0/259.0\n",
      "\n",
      "Updates 102, num timesteps 12360, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 235.7/230.0, min/max reward 218.0/259.0\n",
      "\n",
      "Updates 104, num timesteps 12600, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 106, num timesteps 12840, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 218.0/218.0, min/max reward 218.0/218.0\n",
      "\n",
      "Updates 108, num timesteps 13080, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 251.7/218.0, min/max reward 218.0/319.0\n",
      "\n",
      "Updates 110, num timesteps 13320, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 261.3/235.0, min/max reward 230.0/319.0\n",
      "\n",
      "Updates 112, num timesteps 13560, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 261.3/235.0, min/max reward 230.0/319.0\n",
      "\n",
      "Updates 114, num timesteps 13800, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 232.7/221.0, min/max reward 218.0/259.0\n",
      "\n",
      "Updates 116, num timesteps 14040, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 232.7/221.0, min/max reward 218.0/259.0\n",
      "\n",
      "Updates 118, num timesteps 14280, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 231.7/218.0, min/max reward 218.0/259.0\n",
      "\n",
      "Updates 120, num timesteps 14520, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 222.7/220.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 122, num timesteps 14760, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 222.7/220.0, min/max reward 218.0/230.0\n",
      "\n",
      "Updates 124, num timesteps 15000, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 239.3/218.0, min/max reward 218.0/282.0\n",
      "\n",
      "Updates 126, num timesteps 15240, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 239.3/218.0, min/max reward 218.0/282.0\n",
      "\n",
      "Updates 128, num timesteps 15480, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 282.3/218.0, min/max reward 218.0/411.0\n",
      "\n",
      "Updates 130, num timesteps 15720, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 305.0/283.0, min/max reward 221.0/411.0\n",
      "\n",
      "Updates 132, num timesteps 15960, FPS 86 \n",
      " Last 3 training episodes: mean/median reward 305.0/283.0, min/max reward 221.0/411.0\n",
      "\n",
      "Updates 134, num timesteps 16200, FPS 86 \n",
      " Last 3 training episodes: mean/median reward 243.7/244.0, min/max reward 221.0/266.0\n",
      "\n",
      "Updates 136, num timesteps 16440, FPS 86 \n",
      " Last 3 training episodes: mean/median reward 243.7/244.0, min/max reward 221.0/266.0\n",
      "\n",
      "Updates 138, num timesteps 16680, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 239.7/244.0, min/max reward 221.0/254.0\n",
      "\n",
      "Updates 140, num timesteps 16920, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 267.3/254.0, min/max reward 230.0/318.0\n",
      "\n",
      "Updates 142, num timesteps 17160, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 267.3/254.0, min/max reward 230.0/318.0\n",
      "\n",
      "Updates 144, num timesteps 17400, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 267.0/287.0, min/max reward 218.0/296.0\n",
      "\n",
      "Updates 146, num timesteps 17640, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 267.0/287.0, min/max reward 218.0/296.0\n",
      "\n",
      "Updates 148, num timesteps 17880, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 267.0/287.0, min/max reward 218.0/296.0\n",
      "\n",
      "Updates 150, num timesteps 18120, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 227.7/230.0, min/max reward 218.0/235.0\n",
      "\n",
      "Updates 152, num timesteps 18360, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 227.7/230.0, min/max reward 218.0/235.0\n",
      "\n",
      "Updates 154, num timesteps 18600, FPS 85 \n",
      " Last 3 training episodes: mean/median reward 232.3/221.0, min/max reward 218.0/258.0\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 253, in <module>\n",
      "ShmemVecEnv worker: got KeyboardInterrupt\n",
      "    main()\n",
      "  File \"main.py\", line 160, in main\n",
      "ShmemVecEnv worker: got KeyboardInterrupt\n",
      "ShmemVecEnv worker: got KeyboardInterrupt\n",
      "    obs, reward, done, infos = envs.step(action)\n",
      "  File \"/home/peli/retro-copy/baselines/baselines/common/vec_env/vec_env.py\", line 108, in step\n",
      "    return self.step_wait()\n",
      "  File \"/home/peli/retro-copy/MarioWM/a2c_ppo_acktr/envs.py\", line 386, in step_wait\n",
      "    obs, rews, news, infos = self.venv.step_wait()\n",
      "  File \"/home/peli/retro-copy/MarioWM/a2c_ppo_acktr/envs.py\", line 315, in step_wait\n",
      "    obs, reward, done, info = self.venv.step_wait()\n",
      "  File \"/home/peli/retro-copy/baselines/baselines/common/vec_env/shmem_vec_env.py\", line 76, in step_wait\n",
      "    outs = [pipe.recv() for pipe in self.parent_pipes]\n",
      "  File \"/home/peli/retro-copy/baselines/baselines/common/vec_env/shmem_vec_env.py\", line 76, in <listcomp>\n",
      "    outs = [pipe.recv() for pipe in self.parent_pipes]\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py  --trace-size 1 --rtrace-length 500  --model 0 --weird --autoshroom --episode-length 200 --level 1  --skip  --num-processes 3 --algo ppo  --recurrent-policy --use-linear-lr-decay --use-gae --lr 2.5e-5 --clip-param 0.1 --value-loss-coef 0.5 --env-name \"Mario-v1\" --num-steps 40 --num-mini-batch 1 --log-interval 2  --entropy-coef 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
